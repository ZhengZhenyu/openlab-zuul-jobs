- hosts: all
  become: yes
  roles:
    - clone-devstack-gate-to-workspace
    - create-devstack-local-conf
    - install-devstack
  tasks:
    - shell:
        cmd: |
          # NOTE: the following commands may include sensitive information please do not print in job logs
          # Use echo to delete extra spaces
          export OS_AUTH_TYPE=$(echo '{{ vexxhost_credentials.auth_type }}')
          export OS_IDENTITY_API_VERSION=$(echo '{{ vexxhost_credentials.identity_api_version }}')
          export OS_VOLUME_API_VERSION=$(echo '{{ vexxhost_credentials.volume_api_version }}')
          export OS_INTERFACE=$(echo '{{ vexxhost_credentials.interface }}')
          export OS_AUTH_URL=$(echo '{{ vexxhost_credentials.auth_url }}')
          export OS_PROJECT_ID=$(echo '{{ vexxhost_credentials.project_id }}')
          export OS_PROJECT_NAME=$(echo '{{ vexxhost_credentials.project_name }}')
          export OS_USER_DOMAIN_NAME=$(echo '{{ vexxhost_credentials.user_domain_name }}')
          export OS_PROJECT_DOMAIN_ID=$(echo '{{ vexxhost_credentials.project_domain_id }}')
          export OS_USERNAME=$(echo '{{ vexxhost_credentials.username }}')
          export OS_PASSWORD=$(echo '{{ vexxhost_credentials.password }}')
          export OS_REGION_NAME=$(echo '{{ vexxhost_credentials.region_name }}')

          set -x
          set -e
          set -o pipefail

          # Install docker
          apt update -y
          apt install -y docker.io

          # Custom Docker daemon options via drop-in file then restart
          mkdir /etc/systemd/system/docker.service.d
          cat << EOF > /etc/systemd/system/docker.service.d/local-up-cluster.conf
          [Service]
          Environment="DOCKER_OPTS=--iptables=false"
          EOF
          systemctl daemon-reload
          systemctl restart docker

          # Install etcd
          wget https://github.com/coreos/etcd/releases/download/v3.3.0/etcd-v3.3.0-linux-amd64.tar.gz
          tar -zxf etcd-v3.3.0-linux-amd64.tar.gz
          cp etcd-v3.3.0-linux-amd64/etcd{,ctl} /usr/local/bin/

          # Install dependencies
          go get -u github.com/Masterminds/glide

          # Build binaries
          make build

          # Build k8s cmd
          git clone https://github.com/kubernetes/kubernetes ${GOPATH}/src/k8s.io/kubernetes
          # Hack to be compatible with all clouds
          sed -i 's/2016-06-30/latest/' ${GOPATH}/src/k8s.io/kubernetes/pkg/cloudprovider/providers/openstack/openstack_volumes.go
          sed -i 's/curl --max-time 1/curl --max-time 5/' ${GOPATH}/src/k8s.io/kubernetes/hack/lib/util.sh
          make -C ${GOPATH}/src/k8s.io/kubernetes WHAT="cmd/kubectl cmd/hyperkube"

          # Create cloud-config
          mkdir -p /etc/kubernetes/
          cat << EOF >> /etc/kubernetes/cloud-config
          [Global]
          domain-name = ${OS_PROJECT_DOMAIN_NAME-$OS_PROJECT_DOMAIN_ID}
          tenant-id = $OS_PROJECT_ID
          auth-url = $OS_AUTH_URL
          password = $OS_PASSWORD
          username = $OS_USERNAME
          region = $OS_REGION_NAME

          [BlockStorage]
          bs-version = v2
          ignore-volume-az = yes
          EOF

          # Stopping firewall and allow all traffic
          iptables -F
          iptables -X
          iptables -t nat -F
          iptables -t nat -X
          iptables -t mangle -F
          iptables -t mangle -X
          iptables -P INPUT ACCEPT
          iptables -P FORWARD ACCEPT
          iptables -P OUTPUT ACCEPT

          # Go where we cloned kubernetes repository
          cd $GOPATH/src/k8s.io/kubernetes/

          export API_HOST_IP=$(ifconfig | awk '/docker0/ {getline; print $2}' | awk -F ':' '{print $2}')
          export KUBELET_HOST="0.0.0.0"
          export ALLOW_SECURITY_CONTEXT=true
          export ENABLE_CRI=false
          export ENABLE_HOSTPATH_PROVISIONER=true
          export ENABLE_SINGLE_CA_SIGNER=true
          export KUBE_ENABLE_CLUSTER_DNS=false
          export LOG_LEVEL=4
          # We want to use the openstack cloud provider
          export CLOUD_PROVIDER=openstack
          # We want to run a separate cloud-controller-manager for openstack
          export EXTERNAL_CLOUD_PROVIDER=true
          # DO NOT change the location of the cloud-config file. It is important for the old cinder provider to work
          export CLOUD_CONFIG=/etc/kubernetes/cloud-config
          # Specify the OCCM binary
          export EXTERNAL_CLOUD_PROVIDER_BINARY='{{ ansible_user_dir }}/{{ zuul.project.src_dir }}/openstack-cloud-controller-manager'

          # location of where the kubernetes processes log their output
          mkdir -p /opt/stack/logs/
          export LOG_DIR=/opt/stack/logs
          # We need this for one of the conformance tests
          export ALLOW_PRIVILEGED=true
          # Just kick off all the processes and drop down to the command line
          export ENABLE_DAEMON=true
          # We need the hostname to match the name of the vm started by openstack
          export HOSTNAME_OVERRIDE=$(curl http://169.254.169.254/openstack/latest/meta_data.json | python -c "import sys, json; print json.load(sys.stdin)['name']")

          # -E preserves the current env vars, but we need to special case PATH
          sleep 7200
          sudo -E PATH=$PATH SHELLOPTS=$SHELLOPTS ./hack/local-up-cluster.sh -O
          nohup '{{ ansible_user_dir }}/{{ zuul.project.src_dir }}/cinder-provisioner' --cloud-config "$CLOUD_CONFIG" --kubeconfig /var/run/kubernetes/controller.kubeconfig --id cinder > "$LOG_DIR/cinder-provisioner.log" 2>&1 &


          # set up the config we need for kubectl to work
          cluster/kubectl.sh config set-cluster local --server=https://localhost:6443 --certificate-authority=/var/run/kubernetes/server-ca.crt
          cluster/kubectl.sh config set-credentials myself --client-key=/var/run/kubernetes/client-admin.key --client-certificate=/var/run/kubernetes/client-admin.crt
          cluster/kubectl.sh config set-context local --cluster=local --user=myself
          cluster/kubectl.sh config use-context local

          # Hack for RBAC for all for the new cloud-controller process, we need to do better than this
          cluster/kubectl.sh create clusterrolebinding --user system:serviceaccount:kube-system:default kube-system-cluster-admin-1 --clusterrole cluster-admin
          cluster/kubectl.sh create clusterrolebinding --user system:serviceaccount:kube-system:pvl-controller kube-system-cluster-admin-2 --clusterrole cluster-admin
          cluster/kubectl.sh create clusterrolebinding --user system:serviceaccount:kube-system:cloud-node-controller kube-system-cluster-admin-3 --clusterrole cluster-admin
          cluster/kubectl.sh create clusterrolebinding --user system:serviceaccount:kube-system:cloud-controller-manager kube-system-cluster-admin-4 --clusterrole cluster-admin
          cluster/kubectl.sh create clusterrolebinding --user system:serviceaccount:kube-system:shared-informers kube-system-cluster-admin-5 --clusterrole cluster-admin
          cluster/kubectl.sh create clusterrolebinding --user system:kube-controller-manager  kube-system-cluster-admin-6 --clusterrole cluster-admin

          # Remove node taint
          cluster/kubectl.sh delete storageclass standard
          # cluster/kubectl.sh label nodes "$HOSTNAME_OVERRIDE" failure-domain.beta.kubernetes.io/zone=ca-ymq-2

          # Create secret used by pv
          cluster/kubectl.sh create -f - << EOF
          apiVersion: v1
          kind: Secret
          metadata:
            name: standard-cephx-secret
            namespace: default
          data:
            token: c2VjcmV0Cg==
          EOF

          # Cinder test
          cluster/kubectl.sh apply -f '{{ ansible_user_dir }}/{{ zuul.project.src_dir }}/examples/persistent-volume-provisioning/cinder/cinder-full.yaml'

          # If test passed
          timeout 100 bash -c '
              while :
              do
                  [[ $(cluster/kubectl.sh describe pods web | awk '/^Status:/ {print $2}') == Running ]] && break
                  sleep 5
              done
              '
        executable: /bin/bash
        chdir: '{{ zuul.project.src_dir }}'
      environment: '{{ golang_env }}'
