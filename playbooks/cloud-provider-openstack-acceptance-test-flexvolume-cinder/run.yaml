- hosts: all
  become: yes
  roles:
    - clone-devstack-gate-to-workspace
    - create-devstack-local-conf
    - role: install-devstack
      environment:
        OVERRIDE_ENABLED_SERVICES: 'key,c-sch,c-api,c-vol,rabbit,mysql'
  tasks:
    - shell:
        cmd: |
          set -x
          # set -e
          # set -o pipefail

          # Create cloud-config
          source /opt/stack/new/devstack/openrc admin admin
          mkdir -p /etc/kubernetes/
          cat << EOF >> /etc/kubernetes/cloud-config
          [Global]
          domain-id = $OS_PROJECT_DOMAIN_ID
          tenant-name = $OS_TENANT_NAME
          auth-url = $OS_AUTH_URL
          password = $OS_PASSWORD
          username = $OS_USERNAME
          region = $OS_REGION_NAME

          [BlockStorage]
          bs-version = v2
          ignore-volume-az = yes
          EOF

          export API_HOST_IP=$(ifconfig | awk '/^docker0/ {getline; print $2}' | awk -F ':' '{print $2}')
          export KUBELET_HOST="0.0.0.0"
          export ALLOW_SECURITY_CONTEXT=true
          export ENABLE_CRI=false
          export ENABLE_HOSTPATH_PROVISIONER=true
          export ENABLE_SINGLE_CA_SIGNER=true
          export KUBE_ENABLE_CLUSTER_DNS=false
          export LOG_LEVEL=7
          # DO NOT change the location of the cloud-config file. It is important for the old cinder provider to work
          export CLOUD_CONFIG=/etc/kubernetes/cloud-config

          # location of where the kubernetes processes log their output
          mkdir -p '{{ ansible_user_dir }}/workspace/logs/kubernetes'
          export LOG_DIR='{{ ansible_user_dir }}/workspace/logs/kubernetes'
          # We need this for one of the conformance tests
          export ALLOW_PRIVILEGED=true
          # Just kick off all the processes and drop down to the command line
          export ENABLE_DAEMON=true
          export HOSTNAME_OVERRIDE=127.0.0.1
          export MAX_TIME_FOR_URL_API_SERVER=5
          export RUNTIME_CONFIG="settings.k8s.io/v1alpha1=true"

          # Workaround to rebuild cinder-flex-volume-driver binary using patched code
          cp -f pkg/flexvolume/cinder_client.go vendor/k8s.io/cloud-provider-openstack/pkg/flexvolume/cinder_client.go
          cp -f pkg/flexvolume/cinder_baremetal_util.go vendor/k8s.io/cloud-provider-openstack/pkg/flexvolume/cinder_baremetal_util.go
          cd /home/zuul/src/k8s.io/cloud-provider-openstack && CGO_ENABLED=0 GOOS=linux go build \
              -ldflags "-X 'main.version=19c9e833-dirty'" \
              -o cinder-flex-volume-driver \
              cmd/cinder-flex-volume-driver/main.go
          cd -

          mkdir -p /usr/libexec/kubernetes/kubelet-plugins/volume/exec/cinder.io~cinder-flex-volume-driver/
          cp cinder-flex-volume-driver /usr/libexec/kubernetes/kubelet-plugins/volume/exec/cinder.io~cinder-flex-volume-driver/

          export k8s_root="$GOPATH/src/k8s.io/kubernetes"
          export kubectl="$k8s_root/cluster/kubectl.sh"
          # -E preserves the current env vars, but we need to special case PATH
          sudo -E PATH=$PATH SHELLOPTS=$SHELLOPTS "$k8s_root/hack/local-up-cluster.sh" -O

          # set up the config we need for kubectl to work
          "$kubectl" config set-cluster local --server=https://localhost:6443 --certificate-authority=/var/run/kubernetes/server-ca.crt
          "$kubectl" config set-credentials myself --client-key=/var/run/kubernetes/client-admin.key --client-certificate=/var/run/kubernetes/client-admin.crt
          "$kubectl" config set-context local --cluster=local --user=myself
          "$kubectl" config use-context local

          # Hack for RBAC for all for the new cloud-controller process, we need to do better than this
          "$kubectl" create clusterrolebinding --user system:serviceaccount:kube-system:default kube-system-cluster-admin-1 --clusterrole cluster-admin
          "$kubectl" create clusterrolebinding --user system:serviceaccount:kube-system:pvl-controller kube-system-cluster-admin-2 --clusterrole cluster-admin
          "$kubectl" create clusterrolebinding --user system:serviceaccount:kube-system:cloud-node-controller kube-system-cluster-admin-3 --clusterrole cluster-admin
          "$kubectl" create clusterrolebinding --user system:serviceaccount:kube-system:cloud-controller-manager kube-system-cluster-admin-4 --clusterrole cluster-admin
          "$kubectl" create clusterrolebinding --user system:serviceaccount:kube-system:shared-informers kube-system-cluster-admin-5 --clusterrole cluster-admin
          "$kubectl" create clusterrolebinding --user system:kube-controller-manager  kube-system-cluster-admin-6 --clusterrole cluster-admin

          # Run test
          VOLUME_ID=$(cinder create 1 | awk '/ id / {print $4}')
          cat << EOF | "$kubectl" create -f -
          apiVersion: v1
          kind: Pod
          metadata:
            name: nginx
            namespace: default
          spec:
            containers:
            - name: nginx
              image: nginx
              volumeMounts:
              - name: test
                mountPath: /data
              ports:
              - containerPort: 80
            volumes:
            - name: test
              flexVolume:
                driver: "cinder.io/cinder-flex-volume-driver"
                fsType: "ext4"
                options:
                  volumeID: "$VOLUME_ID"
                  cinderConfig: "/etc/kubernetes/cloud-config"
          EOF

          # If test passed
          if timeout 100 bash -c '
              while :
              do
                  [[ $("$kubectl" describe pod nginx | awk "/^Status:/ {print \$2}") == Running ]] && break
                  sleep 1
              done
              '
          then
              echo 'Run test successful'
              "$kubectl" get pod nginx
          else
              echo 'Run test failed'
              "$kubectl" get pod nginx
              "$kubectl" describe pod nginx
              # exit 1
          fi

          sleep 7200
        executable: /bin/bash
        chdir: '{{ zuul.project.src_dir }}'
      environment: '{{ golang_env }}'
